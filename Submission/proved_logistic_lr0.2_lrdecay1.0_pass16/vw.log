 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=2 | AP@k=0.77711692
 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=4 | AP@k=0.7773937
 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=6 | AP@k=0.7773915
 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=8 | AP@k=0.77738286
 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=10 | AP@k=0.77738128
 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=12 | AP@k=0.77737641
 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=14 | AP@k=0.77737611
 * Train for: loss=logistic, lr=0.5, lr_decay=0.5, passes=16 | AP@k=0.77737599
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=2 | AP@k=0.77766678
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=4 | AP@k=0.77825769
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=6 | AP@k=0.77830937
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=8 | AP@k=0.77831748
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=10 | AP@k=0.7783178
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=12 | AP@k=0.77831013
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=14 | AP@k=0.77830398
 * Train for: loss=logistic, lr=0.5, lr_decay=0.75, passes=16 | AP@k=0.77829761
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=2 | AP@k=0.77807354
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=4 | AP@k=0.77880997
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=6 | AP@k=0.77887214
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=8 | AP@k=0.77881132
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=10 | AP@k=0.77873892
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=12 | AP@k=0.77867083
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=14 | AP@k=0.77859887
 * Train for: loss=logistic, lr=0.5, lr_decay=1.0, passes=16 | AP@k=0.77853
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=2 | AP@k=0.77140903
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=4 | AP@k=0.7726372
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=6 | AP@k=0.77287524
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=8 | AP@k=0.77292698
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=10 | AP@k=0.77293814
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=12 | AP@k=0.77293967
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=14 | AP@k=0.77293978
 * Train for: loss=logistic, lr=0.2, lr_decay=0.5, passes=16 | AP@k=0.77293893
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=2 | AP@k=0.77258128
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=4 | AP@k=0.7748917
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=6 | AP@k=0.77560872
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=8 | AP@k=0.775914
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=10 | AP@k=0.77606329
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=12 | AP@k=0.7761382
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=14 | AP@k=0.77617776
 * Train for: loss=logistic, lr=0.2, lr_decay=0.75, passes=16 | AP@k=0.77619941
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=2 | AP@k=0.7735351
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=4 | AP@k=0.77679801
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=6 | AP@k=0.77798959
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=8 | AP@k=0.77855941
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=10 | AP@k=0.77889624
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=12 | AP@k=0.77910547
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=14 | AP@k=0.77923612
 * Train for: loss=logistic, lr=0.2, lr_decay=1.0, passes=16 | AP@k=0.77933152
 * Best params for loss=logistic:
   lr=0.2, lr_decay=1.0, passes=16 | AP@k=0.77933152
